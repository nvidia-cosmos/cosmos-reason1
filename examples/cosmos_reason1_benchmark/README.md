# Cosmos Reason1 Benchmark

This guide provides instructions for evaluating models on the [Cosmos Reason1 Benchmark](https://huggingface.co/datasets/nvidia/Cosmos-Reason1-Benchmark)

## Download Dataset

Download annotations and sample video clips using the script below:

```bash
python tools/eval/download_hf_data.py \
    --target data \
    --task benchmark
```

> **Note:**
> This script downloads:
>
> - ‚úÖ Annotations for:
>   - `AV` # For autonomous vehicles' general description, driving difficulty, and notice
>   - [RoboVQA](https://robovqa.github.io/) # Videos, instructions, and question-answer pairs of agents (robots, humans, humans-with-grasping-tools) executing a task.
>   - [AgiBot-World](https://github.com/OpenDriveLab/AgiBot-World) # A wide range of real-life tasks for robot manipulation
>   - [BridgeData V2](https://rail-berkeley.github.io/bridgedata/) # A wide array of robotic manipulation behaviors
>   - [HoloAssist Dataset](https://holoassist.github.io/) # Crucial first-person perspectives that provide natural and immersive understanding of human actions
> - ‚úÖ Video clips for:
>   - `AV`
>   - `RoboVQA`
>   - ‚ö†Ô∏è Video clips for AgiBot-World, BridgeData V2, and HoloAssist must be downloaded manually in the next step (optional).

### üõ†Ô∏è Step 2: (Optional) Download Remaining Video Clips

#### üì• Get Access for AgiBot

- **Dataset**: [AgiBotWorld-Beta on Hugging Face](https://huggingface.co/datasets/agibot-world/AgiBotWorld-Beta/tree/main)

#### ‚öôÔ∏è Run Preprocessing Script

Run the following script to download and preprocess video clips, take `holoassist` as example:

```bash
# Export HF_TOKEN to get access to Cosmos Reason dataset
export HF_TOKEN=...

python tools/eval/process_raw_data.py \
  --dataset holoassist \
  --data_dir data \
  --task benchmark
```

> üí° Replace `holoassist` with `agibot` or `bridgev2` as needed.

### üöÄ Step 3: Run Evaluation on Benchmarks

This step walks you through running evaluations on your model using the provided script.

#### üìñ Configure Evaluation

You can configure evaluation settings by editing yaml file under `tools/eval/configs/`, take `robovqa.yaml` as example:

```yaml
datasets:
  - robovqa

model:
  model_name: nvidia/Cosmos-Reason1-7B # You can also replace the model_name by a safetensors folder path mentioned above
  tokenizer_model_name: qwen2.5-vl-7b
  dtype: bfloat16
  tp_size: 4
  max_length: 128000

evaluation:
  answer_type: reasoning
  num_processes: 80
  skip_saved: false
  fps: 4
  seed: 1

generation:
  max_retries: 10
  max_tokens: 1024
  temperature: 0.6
  repetition_penalty: 1.0
  presence_penalty: 0.0
  frequency_penalty: 0.0
```

#### üîç Run Evaluation

Run the evaluation on the **RoboVQA** dataset:

```bash
# Set tensor parallelism size (adjust as needed)
export TP_SIZE=4

# Run the evaluation script
PYTHONPATH=. python3 tools/eval/evaluate.py \
    --config tools/eval/configs/robovqa.yaml \
    --data_dir data \
    --results_dir results
```

*Tip:* You can also use `--model_name` to specify either a Hugging Face model name or a local safetensors folder path mentioned above.

### üìä Step 4: Benchmark Scoring

This step computes benchmark accuracy metrics from prediction results stored in a specified directory. It is used to evaluate model performance on datasets such as **RoboVQA**.

#### üìã About the Evaluation

The evaluation uses **accuracy** as the primary metric, comparing model predictions against ground-truth answers. Accuracy is computed as:

> **Accuracy = (# correct predictions) / (total questions)**

For open-ended questions, a prediction is considered correct if it exactly matches the ground truth (case-insensitive string match). For multiple-choice questions, the selected option is compared against the correct choice.

> ‚ö†Ô∏è **Note:** These scoring rules follow common practices in VLM QA literature, but users are encouraged to adapt or extend them for specific use cases (e.g., partial credit, VQA-style soft accuracy).

#### üîß Usage

Run the following command to compute accuracy:

```bash
python tools/eval/calculate_accuracy.py --result_dir results --dataset robovqa
```

- `--result_dir`: Path to the directory containing the model's prediction results. This should match the `--result_dir` used during evaluation in `evaluate.py`.
- `--dataset`: Name of the dataset to evaluate (e.g., `robovqa`, `av`, `agibot`, `bridgev2`, `holoassist`).
